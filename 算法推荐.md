- 问题一模型求解推荐算法：
  - Pandas条件查询
    - 原理：利用Pandas的布尔索引机制，通过多条件组合（轴承类型、故障类型、转速范围、故障直径）过滤样本，例如`df[(df['bearing_type'].isin(['SKF6205', 'SKF6203'])) & (df['rpm'].between(1000, 3000)) & (df['fault_diameter'].isin([0.007, 0.014, 0.021]))]`。
    - 理由：Pandas是Python生态中数据预处理的标准工具，条件查询语法直观、运算高效，能快速处理161个源域样本的筛选需求，且支持与后续特征提取步骤的无缝衔接。
  - Numpy向量化运算
    - 原理：基于Numpy的`max`、`sqrt`、`sum`、`mean`、`std`等函数计算时域特征：峰值为`np.max(np.abs(acceleration))`，RMS为`np.sqrt(np.mean(acceleration**2))`，峰值因子为`峰值/RMS`，脉冲因子为`峰值/np.mean(np.abs(acceleration))`，峭度为`(np.mean((acceleration - μ) ** 4) / σ ** 4) - 3`（其中μ为均值，σ为标准差）。
    - 理由：Numpy的向量化运算避免了循环的低效性，适合处理48000点的长时域序列；时域特征均为统计量计算，Numpy原生函数可直接实现，无需额外依赖。
  - FFT（快速傅里叶变换）
    - 原理：使用Numpy的`fft.fft`对时域信号做傅里叶变换，得到频域幅值谱`abs(fft_result)`；根据理论特征频率（BPFO/BPFI/BSF），提取±1Hz范围内的最大幅值`np.max(abs(fft_result)[freq_range])`，并计算1~3次谐波的幅值和`np.sum(abs(fft_result)[[k*FF for k in 1:3]])`。
    - 理由：FFT是频域分析的经典方法，能快速将时域信号转换为频域表示；频率分辨率0.25Hz（48000点）足够捕捉特征频率的细微变化，符合问题一的频域特征约束。
  - PyWavelets小波包分解
    - 原理：使用PyWavelets的`WaveletPacket`类，以db4为小波基对信号做3层分解，得到各节点的系数；计算前4个低频节点的能量`np.sum(coeff**2)`，并归一化为能量占比`能量/总能量`。
    - 理由：小波包分解能同时分析信号的时间和频率局部特性，适合非平稳的振动信号；db4小波基兼具正交性与紧支性，对冲击信号的分解效果好。
  - ReliefF算法
    - 原理：通过计算特征对同类/异类样本的区分能力更新权重（对每个样本找k个同类近邻和k个异类近邻，特征权重=权重-同类差异均值+异类差异均值），最终按权重排序选取前10维特征。
    - 理由：ReliefF是针对分类问题的过滤式特征选择算法，能有效筛选出对故障类型判别有用的特征；避免高维特征导致的“维度灾难”，为后续迁移诊断任务减少计算量。

- 问题二模型求解推荐算法：
  - StratifiedShuffleSplit（sklearn）
    - 原理：对每个故障类型k，按70%比例随机选择样本组成训练集，剩余样本组成测试集，保持各类别样本比例一致；设置`random_state=42`保证结果可重复。
    - 理由：分层抽样避免了随机抽样可能导致的类别不平衡，确保训练集与测试集的分布一致，符合问题二的“分层抽样”要求；sklearn的`StratifiedShuffleSplit`直接实现该逻辑，无需手动处理。
  - StandardScaler（sklearn）
    - 原理：计算训练集特征的均值`μ_f`和标准差`σ_f`，对训练集/测试集做标准化`x_scaled = (x - μ_f) / σ_f`（测试集使用训练集的`μ_f`和`σ_f`，避免数据泄漏）。
    - 理由：标准化消除了特征的量纲差异（如“峰值”单位是g，“能量占比”是无量纲），使神经网络的输入尺度一致，加快收敛速度；`StandardScaler`是sklearn的原生工具，使用简便。
  - Keras/TensorFlow构建1D-CNN
    - 原理：构建1D-CNN模型：输入层`(13, 1)`（将13维特征视为1D序列，通道数为1）→卷积层（核大小3，输出通道数64，ReLU激活）→池化层（1D最大池化，窗口大小2）→扁平化层→全连接层（320→32→4，ReLU/Softmax激活）；用Adam优化器（`lr=0.001`）、交叉熵损失函数训练100轮。
    - 理由：1D-CNN适合处理序列特征（问题二的13维特征是“多域特征序列”），卷积层能提取特征间的局部关联；Keras是高层API，代码简洁，容易调整模型结构（如增加卷积层/通道数）。
  - Classification_report与Confusion_matrix（sklearn）
    - 原理：`classification_report`计算每个故障类型的精确率、召回率、F1-score；`confusion_matrix`展示真实标签与预测标签的混淆情况（如内圈故障被误判为外圈故障的数量）。
    - 理由：这些工具能快速生成全面的评价指标，直观反映模型性能；例如，F1-score平衡了精确率与召回率，适合类别不平衡的情况（如健康样本可能较少）。

- 问题三模型求解推荐算法（改用TCA）：
  - 目标域特征处理（沿用问题一与问题二的算法）
    - 原理：目标域数据用问题一的特征提取方法（时域、频域、时频域）得到13维特征，再用问题二的`StandardScaler`（源域的`μ_f`和`σ_f`）标准化。
    - 理由：保持源域与目标域的特征维度（13维）和尺度一致，确保TCA的输入具有可比性；避免特征尺度差异导致的分布偏移，符合“域不变特征”的学习前提。
  - TCA（Transfer Component Analysis）
    - 原理：学习一个线性映射`φ`，将源域`X_s`与目标域`X_t`的特征映射到低维子空间（如64维）；优化目标为`min (L_cls + λ · MMD(φ(X_s), φ(X_t)))`，其中`L_cls`是源域的分类损失（如逻辑回归的交叉熵损失），`MMD`是最大均值差异（`MMD = ||E[φ(X_s)] - E[φ(X_t)]||²`），`λ`是权衡分类性能与域适应的参数（如`λ=1`）。
    - 理由：TCA是经典的迁移学习算法，针对“源域有标签、目标域无标签”的场景；通过MMD最小化分布差异，学习域不变特征，解决问题三的“工况不同导致的特征分布偏移”问题（如转速2500rpm的目标域与1797rpm的源域）；符合用户“改用TCA”的要求。
  - 逻辑回归分类器（sklearn）
    - 原理：用映射后的源域特征`φ(X_s)`训练逻辑回归分类器；将目标域映射后的特征`φ(X_t)`输入分类器，得到预测标签`argmax(预测概率)`。
    - 理由：TCA学习的域不变特征消除了源域与目标域的分布差异，源域分类器可直接泛化到目标域；逻辑回归模型简单，计算快，适合小样本的目标域（16个样本）。

- 问题四模型求解推荐算法：
  - 事前可解释性分析（模型结构与机理关联）
    - 原理：分析TCA的映射`φ`如何融合多域特征（如包含“内圈特征频率幅值（Amp_BPFI）”，对应内圈故障机理：`BPFI = f_r · n/2 · (1 + d/D)`）；标签分类器输出层对应4种故障类型，与工程人员“特征频率→故障类型”的诊断逻辑一致；域分类器通过区分源域与目标域特征，倒逼特征提取器学习域不变特征。
    - 理由：结合轴承几何参数→特征频率→信号特征的机理链，解释TCA模型结构的合理性；避免“黑箱模型”，符合问题四的“事前可解释性”要求。
  - 迁移过程可解释性分析（MMD损失曲线可视化）
    - 原理：绘制MMD损失随迭代次数的变化曲线，初始迭代时MMD较大（源域与目标域分布差异大），随着迭代增加，MMD逐渐减小至接近0.5（域分类器随机猜测）。
    - 理由：MMD曲线直观反映TCA的域适应效果，说明“映射后的特征越来越域不变”；例如，迭代100次后MMD从0.5降至0.1，说明源域与目标域的分布差异显著减小，符合问题四的“迁移过程解释”需求。
  - 事后可解释性分析（SHAP值）
    - 原理：计算目标域样本的SHAP值`φ_f = E[C(G(X_t[j] | X_t[j,f] = x_f))] - E[C(G(X))]`，衡量特征对预测结果的贡献（如内圈故障样本的`Amp_BPFI`特征SHAP值最大且为正）。
    - 理由：SHAP是模型无关的可解释性工具，能定量分析特征重要性；结合轴承故障机理（`Amp_BPFI`对应内圈故障），解释目标域决策的合理性；例如，目标域中某样本的`Amp_BPFI=0.8`（远高于源域内圈故障的均值0.5），其SHAP值为0.6，说明该特征主导了“内圈故障”的预测。
  - 特征对齐可视化（PCA降维）
    - 原理：用PCA将映射后的源域`φ(X_s)`与目标域`φ(X_t)`特征降维到2维，绘制散点图（源域用蓝色，目标域用红色），观察两类点的重叠程度（重叠越多，域不变性越好）。
    - 理由：PCA可视化直观展示域分布的对齐情况；例如，降维后源域与目标域的点大部分重叠，说明TCA学习的特征有效消除了分布偏移，符合问题四的“特征对齐解释”需求。
